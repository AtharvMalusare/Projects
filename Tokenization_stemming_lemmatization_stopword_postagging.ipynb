{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNOjJrtCaWh4IYh+EnUNap5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hussain0048/Projects-/blob/master/Tokenization_stemming_lemmatization_stopword_postagging.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Introduction**"
      ],
      "metadata": {
        "id": "BXUn_-wsT7P8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Natural Language Processing (NLP) has become a cornerstone of modern data science, allowing computers to understand, interpret, and respond to human language in a valuable way. Among the fundamental techniques in NLP are tokenization, stemming, lemmatization, stop words removal, and part-of-speech (POS) tagging. This blog will explore these essential processes and how they contribute to effective text analysis."
      ],
      "metadata": {
        "id": "quV2dNtejrRv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Table of Content**"
      ],
      "metadata": {
        "id": "FjGmkrDFjuK-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Install requirements**"
      ],
      "metadata": {
        "id": "sriupXN0UBcA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jysi-ewXToZu",
        "outputId": "55cf98f0-8015-4954-b329-1bcbdd5ab854"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy==1.19.5\n",
            "  Downloading numpy-1.19.5.zip (7.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: numpy\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for numpy \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for numpy (pyproject.toml) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[31m  ERROR: Failed building wheel for numpy\u001b[0m\u001b[31m\n",
            "\u001b[0mFailed to build numpy\n",
            "\u001b[31mERROR: Could not build wheels for numpy, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting nltk==3.2.5\n",
            "  Downloading nltk-3.2.5.tar.gz (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from nltk==3.2.5) (1.16.0)\n",
            "Building wheels for collected packages: nltk\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.2.5-py3-none-any.whl size=1392142 sha256=d46faec1b779fd74e3ca2f5da4ee6b0edee721aaadab0b49f0ed738ccf953bd5\n",
            "  Stored in directory: /root/.cache/pip/wheels/50/d6/35/4a8a48ea9fe03abae30da7971b8ed2a350436bebc00541372b\n",
            "Successfully built nltk\n",
            "Installing collected packages: nltk\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.8.1\n",
            "    Uninstalling nltk-3.8.1:\n",
            "      Successfully uninstalled nltk-3.8.1\n",
            "Successfully installed nltk-3.2.5\n",
            "Collecting spacy==2.2.4\n",
            "  Downloading spacy-2.2.4.tar.gz (6.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.1/6.1 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpip subprocess to install build dependencies\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m \u001b[32mpip subprocess to install build dependencies\u001b[0m did not run successfully.\n",
            "\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n"
          ]
        }
      ],
      "source": [
        "# To install only the requirements of this notebook, uncomment the lines below and run this cell\n",
        "\n",
        "# ===========================\n",
        "\n",
        "!pip install numpy==1.19.5\n",
        "!pip install nltk==3.2.5\n",
        "!pip install spacy==2.2.4\n",
        "\n",
        "# ==========================="
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Dataset**"
      ],
      "metadata": {
        "id": "KsxZ3I-7Wz1E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#This will be our corpus which we will work on\n",
        "corpus_original = \"Need to finalize the demo corpus which will be used for this notebook and it should be done soon !!. It should be done by the ending of this month. But will it? This notebook has been run 4 times !!\"\n",
        "corpus = \"Need to finalize the demo corpus which will be used for this notebook & should be done soon !!. It should be done by the ending of this month. But will it? This notebook has been run 4 times !!\""
      ],
      "metadata": {
        "id": "964twdwyW9S0"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Lower Case**"
      ],
      "metadata": {
        "id": "XLAsWfmtXPYL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One of the simplest yet most important steps in text preprocessing is lowercasing. This step involves converting all the characters in the text to lowercase. Despite its simplicity, lowercasing plays a significant role in ensuring consistent and accurate text analysis"
      ],
      "metadata": {
        "id": "oy2Fa6LPkHNR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#lower case the corpus\n",
        "corpus = corpus.lower()\n",
        "print(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7czXqwcrXMis",
        "outputId": "b0a1cc4a-b4de-47fe-87e5-a47af636f8eb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "need to finalize the demo corpus which will be used for this notebook & should be done soon !!. it should be done by the ending of this month. but will it? this notebook has been run 4 times !!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Removing digits**"
      ],
      "metadata": {
        "id": "oAjafeJ4XdUi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Digits and numbers can add noise to the text data, especially when they are not relevant to the analysis. Removing them can help in focusing on the textual content that carries meaningful information"
      ],
      "metadata": {
        "id": "8FGe7QwZkwKZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#removing digits in the corpus\n",
        "import re\n",
        "corpus = re.sub(r'\\d+','', corpus)\n",
        "print(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DwQICM1-Xmy0",
        "outputId": "0ff99e27-4a4d-4bc1-bc50-29da8d5704de"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "need to finalize the demo corpus which will be used for this notebook & should be done soon !!. it should be done by the ending of this month. but will it? this notebook has been run  times !!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Removing punctuations**"
      ],
      "metadata": {
        "id": "UZispl1xb5et"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Punctuation marks, while essential for human readability, often do not contribute to the meaning in the context of text analysis and can be considered noise in many NLP tasks. Removing punctuation is a common preprocessing step that helps in cleaning and standardizing the text data."
      ],
      "metadata": {
        "id": "B1iiQtJElToR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#removing punctuations\n",
        "import string\n",
        "corpus = corpus.translate(str.maketrans('', '', string.punctuation))\n",
        "print(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PFqRsVf0cDrG",
        "outputId": "b8e120a2-29b2-40c9-a808-b66fdb042bcc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "need to finalize the demo corpus which will be used for this notebook  should be done soon  it should be done by the ending of this month but will it this notebook has been run  times \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Removing trailing whitespaces**"
      ],
      "metadata": {
        "id": "h3lBWlAdcCpN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trailing whitespace can make the text data appear messy and unstructured. Removing it helps in creating a cleaner dataset, which is easier to process and analyze"
      ],
      "metadata": {
        "id": "Quc1ONZtlqN2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample text with trailing whitespace\n",
        "text = \"This is an example sentence with trailing spaces.   \""
      ],
      "metadata": {
        "id": "_frzvdYulrov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = ' '.join([token for token in corpus.split()])\n",
        "corpus"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "_UZvYeWAcUpk",
        "outputId": "6d2d9fb6-6cd1-4937-91ad-606d9d3ffdd1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'need to finalize the demo corpus which will be used for this notebook should be done soon it should be done by the ending of this month but will it this notebook has been run times'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "id": "Uz7ITPqCcZPk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Tokenizing**"
      ],
      "metadata": {
        "id": "BC98rGzzdSMM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint\n",
        "##NLTK\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "stop_words_nltk = set(stopwords.words('english'))\n",
        "\n",
        "tokenized_corpus_nltk = word_tokenize(corpus)\n",
        "print(\"\\nNLTK\\nTokenized corpus:\",tokenized_corpus_nltk)\n",
        "tokenized_corpus_without_stopwords = [i for i in tokenized_corpus_nltk if not i in stop_words_nltk]\n",
        "print(\"Tokenized corpus without stopwords:\",tokenized_corpus_without_stopwords)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0801ZFzLfGcE",
        "outputId": "d1ff335f-7ec1-4354-a169-2e70e3c32c2b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "NLTK\n",
            "Tokenized corpus: ['need', 'to', 'finalize', 'the', 'demo', 'corpus', 'which', 'will', 'be', 'used', 'for', 'this', 'notebook', 'should', 'be', 'done', 'soon', 'it', 'should', 'be', 'done', 'by', 'the', 'ending', 'of', 'this', 'month', 'but', 'will', 'it', 'this', 'notebook', 'has', 'been', 'run', 'times']\n",
            "Tokenized corpus without stopwords: ['need', 'finalize', 'demo', 'corpus', 'used', 'notebook', 'done', 'soon', 'done', 'ending', 'month', 'notebook', 'run', 'times']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##SPACY\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "import spacy\n",
        "spacy_model = spacy.load('en_core_web_sm')\n",
        "\n",
        "stopwords_spacy = spacy_model.Defaults.stop_words\n",
        "print(\"\\nSpacy:\")\n",
        "tokenized_corpus_spacy = word_tokenize(corpus)\n",
        "print(\"Tokenized Corpus:\",tokenized_corpus_spacy)\n",
        "tokens_without_sw= [word for word in tokenized_corpus_spacy if not word in stopwords_spacy]\n",
        "\n",
        "print(\"Tokenized corpus without stopwords\",tokens_without_sw)\n",
        "\n",
        "\n",
        "print(\"Difference between NLTK and spaCy output:\\n\",\n",
        "      set(tokenized_corpus_without_stopwords)-set(tokens_without_sw))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PRm1CKf4fUas",
        "outputId": "b8640701-2bc2-4cb0-8ae8-445b22c2d415"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Spacy:\n",
            "Tokenized Corpus: ['need', 'to', 'finalize', 'the', 'demo', 'corpus', 'which', 'will', 'be', 'used', 'for', 'this', 'notebook', 'should', 'be', 'done', 'soon', 'it', 'should', 'be', 'done', 'by', 'the', 'ending', 'of', 'this', 'month', 'but', 'will', 'it', 'this', 'notebook', 'has', 'been', 'run', 'times']\n",
            "Tokenized corpus without stopwords ['need', 'finalize', 'demo', 'corpus', 'notebook', 'soon', 'ending', 'month', 'notebook', 'run', 'times']\n",
            "Difference between NLTK and spaCy output:\n",
            " {'used', 'done'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Stemming**"
      ],
      "metadata": {
        "id": "2T7HDu2BgVxH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "stemmer= PorterStemmer()\n",
        "\n",
        "print(\"Before Stemming:\")\n",
        "print(corpus)\n",
        "\n",
        "print(\"After Stemming:\")\n",
        "for word in tokenized_corpus_nltk:\n",
        "    print(stemmer.stem(word),end=\" \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pe5XAdgqgk64",
        "outputId": "70d43b48-9b23-492b-a859-8482e6765c2b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before Stemming:\n",
            "need to finalize the demo corpus which will be used for this notebook should be done soon it should be done by the ending of this month but will it this notebook has been run times\n",
            "After Stemming:\n",
            "need to final the demo corpu which will be use for thi notebook should be done soon it should be done by the end of thi month but will it thi notebook ha been run time "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Lemmatization**"
      ],
      "metadata": {
        "id": "jmqhcKhEgqOr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0jf3VLxug2tk",
        "outputId": "6a8b881d-4fc6-4a68-b34f-07cf247c9e77"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -m nltk.downloader wordnet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qDqm5CUUhEV9",
        "outputId": "5b0e0e52-3d35-477f-951f-38e9a38a32a3"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/lib/python3.10/runpy.py:126: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
            "  warn(RuntimeWarning(msg))\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t6VxpnSihaPR",
        "outputId": "17fa0a93-34af-490e-cc5f-545e01f7423c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('wordnet')\n",
        "lemmatizer=WordNetLemmatizer()\n",
        "\n",
        "for word in tokenized_corpus_nltk:\n",
        "    print(lemmatizer.lemmatize(word),end=\" \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KUtJJtAEgwEq",
        "outputId": "c7fb6077-acdf-4358-e770-1a26afac3948"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "need to finalize the demo corpus which will be used for this notebook should be done soon it should be done by the ending of this month but will it this notebook ha been run time "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **POS Tagging**"
      ],
      "metadata": {
        "id": "Jw68KpcFh1MO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#POS tagging using spacy\n",
        "print(\"POS Tagging using spacy:\")\n",
        "doc = spacy_model(corpus_original)\n",
        "# Token and Tag\n",
        "for token in doc:\n",
        "    print(token,\":\", token.pos_)\n",
        "\n",
        "#pos tagging using nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "print(\"POS Tagging using NLTK:\")\n",
        "pprint(nltk.pos_tag(word_tokenize(corpus_original)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kcid9e1Sh78a",
        "outputId": "4dfefcef-f2ca-42df-868b-5c0553df25e0"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POS Tagging using spacy:\n",
            "Need : VERB\n",
            "to : PART\n",
            "finalize : VERB\n",
            "the : DET\n",
            "demo : NOUN\n",
            "corpus : NOUN\n",
            "which : PRON\n",
            "will : AUX\n",
            "be : AUX\n",
            "used : VERB\n",
            "for : ADP\n",
            "this : DET\n",
            "notebook : NOUN\n",
            "and : CCONJ\n",
            "it : PRON\n",
            "should : AUX\n",
            "be : AUX\n",
            "done : VERB\n",
            "soon : ADV\n",
            "! : PUNCT\n",
            "! : PUNCT\n",
            ". : PUNCT\n",
            "It : PRON\n",
            "should : AUX\n",
            "be : AUX\n",
            "done : VERB\n",
            "by : ADP\n",
            "the : DET\n",
            "ending : NOUN\n",
            "of : ADP\n",
            "this : DET\n",
            "month : NOUN\n",
            ". : PUNCT\n",
            "But : CCONJ\n",
            "will : AUX\n",
            "it : PRON\n",
            "? : PUNCT\n",
            "This : DET\n",
            "notebook : NOUN\n",
            "has : AUX\n",
            "been : AUX\n",
            "run : VERB\n",
            "4 : NUM\n",
            "times : NOUN\n",
            "! : PUNCT\n",
            "! : PUNCT\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POS Tagging using NLTK:\n",
            "[('Need', 'NN'),\n",
            " ('to', 'TO'),\n",
            " ('finalize', 'VB'),\n",
            " ('the', 'DT'),\n",
            " ('demo', 'NN'),\n",
            " ('corpus', 'NN'),\n",
            " ('which', 'WDT'),\n",
            " ('will', 'MD'),\n",
            " ('be', 'VB'),\n",
            " ('used', 'VBN'),\n",
            " ('for', 'IN'),\n",
            " ('this', 'DT'),\n",
            " ('notebook', 'NN'),\n",
            " ('and', 'CC'),\n",
            " ('it', 'PRP'),\n",
            " ('should', 'MD'),\n",
            " ('be', 'VB'),\n",
            " ('done', 'VBN'),\n",
            " ('soon', 'RB'),\n",
            " ('!', '.'),\n",
            " ('!', '.'),\n",
            " ('.', '.'),\n",
            " ('It', 'PRP'),\n",
            " ('should', 'MD'),\n",
            " ('be', 'VB'),\n",
            " ('done', 'VBN'),\n",
            " ('by', 'IN'),\n",
            " ('the', 'DT'),\n",
            " ('ending', 'VBG'),\n",
            " ('of', 'IN'),\n",
            " ('this', 'DT'),\n",
            " ('month', 'NN'),\n",
            " ('.', '.'),\n",
            " ('But', 'CC'),\n",
            " ('will', 'MD'),\n",
            " ('it', 'PRP'),\n",
            " ('?', '.'),\n",
            " ('This', 'DT'),\n",
            " ('notebook', 'NN'),\n",
            " ('has', 'VBZ'),\n",
            " ('been', 'VBN'),\n",
            " ('run', 'VBN'),\n",
            " ('4', 'CD'),\n",
            " ('times', 'NNS'),\n",
            " ('!', '.'),\n",
            " ('!', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **References**"
      ],
      "metadata": {
        "id": "M31ZLUHaUi35"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Text pre](https://colab.research.google.com/github/practical-nlp/practical-nlp/blob/master/Ch2/04_Tokenization_Stemming_lemmatization_stopword_postagging.ipynb?authuser=1#scrollTo=gPjHzKkCTJqm)"
      ],
      "metadata": {
        "id": "uOhwy3muU9Ji"
      }
    }
  ]
}