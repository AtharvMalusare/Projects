{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "coursera": {
      "course_slug": "neural-networks-deep-learning",
      "graded_item_id": "XaIWT",
      "launcher_item_id": "zAgPl"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.2"
    },
    "colab": {
      "name": "Building a Real Time Emotion Detection with Python.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hussain0048/Projects-/blob/master/Building_a_Real_Time_Emotion_Detection_with_Python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WZFQQORCAcS",
        "colab_type": "text"
      },
      "source": [
        "# Building a Real Time Emotion Detection with Python\n",
        "# **Introduction:**\n",
        "\n",
        "Detecting real-time emotion of the person with a camera input is one of the advanced features in the machine learning process. The detection of emotion of a person using a camera is useful for various research and analytics purposes. The detection of emotion is made by using the machine learning concept. You can use the trained dataset to detect the emotion of the human being. For detecting the different emotions, first you need to train those different emotions, or you can use a dataset already available on the internet. In this article, we will discuss creating a Python program to detect real-time emotion of a human being using the camera '[1]."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAVZc4ayyDWL",
        "colab_type": "text"
      },
      "source": [
        "# 1-Installing Dependencies "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WpWN1sJv8eho",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install opencv-python"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWA9EJ-zwyej",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tensor flow "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6X6cfXcw4uL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install numpy "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Le9Nov-7w7X6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install pandas  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFMnGbhH9sIS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install keras  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5Pia2KM9x9d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install adam  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnF8AVoy93tt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install kwargs  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dy8GBKBY99lX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install cinit  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnY7VuCDI-qN",
        "colab_type": "text"
      },
      "source": [
        "# 1 **Training the Dataset**#\n",
        "\n",
        "For training purposes, I use the predefined un trained dataset CSV file as my main input for my input for training the machine. You can use the code given below for training the machine using the dataset. Before that, you need to ensure that all required files in the same repository where the program presents otherwise it will through some error. You can download the data set by clicking here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_aCyhDgoTRK",
        "colab_type": "text"
      },
      "source": [
        "## 1.1 What is Computer Vision? \n",
        "Computer Vision is the field of study that enables computers to see and identify digital images and videos as a human would. The challenges it faces largely follow from the limited understanding of biological vision. Computer Vision involves acquiring, processing, analyzing, and understanding digital images to extract high-dimensional data from the real world in order to generate symbolic or numerical information which can then be used to make decisions. The process often includes practices like object recognition, video tracking, motion estimation, and image restoration."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sq4DpmTeRNNy",
        "colab_type": "text"
      },
      "source": [
        "##1.2 What is OpenC(VOpen Source Computer Vision)?\n",
        "OpenCV is short for Open Source Computer Vision. Intuitively by the name, it is an open-source Computer Vision and Machine Learning library. This library is capable of processing real-time image and video while also boasting analytical capabilities. It supports the Deep Learning frameworks TensorFlow, Caffe, and PyTorch.\n",
        "You can perform fast, accurate face detection with OpenCV using a pre-trained deep learning face detector model shipped with the library.\n",
        "\n",
        "You may already know that OpenCV ships out-of-the-box with pre-trained Haar cascades that can be used for face detection…\n",
        "\n",
        "With OpenCV 3.3, we can utilize pre-trained networks with popular deep learning frameworks. The fact that they are pre-trained implies that we don’t need to spend many hours training the network — rather we can complete a forward pass and utilize the output to make a decision within our application [7]\n",
        "\n",
        "With OpenCV 3.3, we can utilize pre-trained networks with popular deep learning frameworks. The fact that they are pre-trained implies that we don’t need to spend many hours training the network — rather we can complete a forward pass and utilize the output to make a decision within our application.\n",
        "\n",
        "OpenCV does not (and does not intend to be) to be a tool for training networks — there are already great frameworks available for that purpose. Since a network (such as a CNN) can be used as a classifier, it makes logical sense that OpenCV has a Deep Learning module that we can leverage easily within the OpenCV ecosystem [7].\n",
        "\n",
        "Popular network architectures compatible with OpenCV 3.3 include:\n",
        "\n",
        "- GoogleLeNet (used in this blog post)\n",
        "- AlexNet\n",
        "- SqueezeNet\n",
        "- VGGNet (and associated flavors)\n",
        "- ResNet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BT1IJkCnjkBb",
        "colab_type": "text"
      },
      "source": [
        "### 1.2.1 dnn Module\n",
        "\n",
        "This module now supports a number of deep learning frameworks, including Caffe, TensorFlow, and Torch/PyTorch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSfzMhlNTrZS",
        "colab_type": "text"
      },
      "source": [
        "### 1.2.1  **Caffe models**\n",
        "When using OpenCV’s deep neural network module with Caffe models, you’ll need two sets of files:\n",
        "\n",
        "The .prototxt file(s) which define the model architecture (i.e., the layers themselves)\n",
        "The .caffemodel file which contains the weights for the actual layers\n",
        "Both files are required when using models trained using Caffe for deep learning.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eV0K0tOIUteD",
        "colab_type": "text"
      },
      "source": [
        "### 1.2.2 How does the OpenCV deep learning face detector work [6]?\n",
        "\n",
        "OpenCV’s deep learning face detector is based on the Single Shot Detector (SSD) framework with a ResNet base network (unlike other OpenCV SSDs that you may have seen which typically use MobileNet as the base network).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcVaahYFRsEF",
        "colab_type": "text"
      },
      "source": [
        "## 1.3 What is a CNN?\n",
        "A Convolutional Neural Network is a deep neural network (DNN) widely used for the purposes of image recognition and processing and NLP. Also known as a ConvNet, a CNN has input and output layers, and multiple hidden layers, many of which are convolutional. In a way, CNNs are regularized multilayer perceptrons.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3C_dTcsnSGFD",
        "colab_type": "text"
      },
      "source": [
        "## 1.4 The CNN Architecture\n",
        "The convolutional neural network for this python project has 3 convolutional layers:\n",
        "- Convolutional layer; 96 nodes, kernel size 7\n",
        "- Convolutional layer; 256 nodes, kernel size 5\n",
        "- Convolutional layer; 384 nodes, kernel size 3\n",
        "\n",
        "It has 2 fully connected layers, each with 512 nodes, and a final output layer of softmax type"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "nhYP15aoCAcf",
        "colab_type": "text"
      },
      "source": [
        "# 2 - **Import library** \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1wsGi52CAch",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys, os  \n",
        "import pandas as pd  \n",
        "import numpy as np "
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfWGnlhx_GYu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential  \n",
        "from keras.layers import Dense, Dropout, Activation, Flatten  \n",
        "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization,AveragePooling2D  \n",
        "from keras.losses import categorical_crossentropy  \n",
        "from keras.optimizers import Adam  \n",
        "from keras.regularizers import l2  \n",
        "from keras.utils import np_utils  \n",
        "# pd.set_option('display.max_rows', 500)  \n",
        "# pd.set_option('display.max_columns', 500)  \n",
        "# pd.set_option('display.width', 1000)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XQFJb0YzEYO",
        "colab_type": "text"
      },
      "source": [
        "#2 - **Load Dataset** # "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hncBMFUXFN6G",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "49456c5d-0098-41b2-bdc1-a6a757c85b74"
      },
      "source": [
        "# this code is used to upload dataset from Pc to colab\n",
        "from google.colab import files # Please First run this cod in chrom \n",
        "def getLocalFiles():\n",
        "    _files = files.upload() # upload StudentNextSessionf.csv datase\n",
        "    if len(_files) >0: # Then run above  libray \n",
        "       for k,v in _files.items():\n",
        "         open(k,'wb').write(v)\n",
        "getLocalFiles()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-f3bbaa62-f76d-4942-b325-6f2c553937bb\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-f3bbaa62-f76d-4942-b325-6f2c553937bb\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving fer2013.csv to fer2013.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPlEL8_ADW3F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget -N https://www.kaggle.com/deadskull7/fer2013"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rqD8iiJZzTZj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df=pd.read_csv('fer2013.csv') "
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpHsrVexXuWj",
        "colab_type": "text"
      },
      "source": [
        "#2-Data Description "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ia4AAHlC6Mhb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(df.info())  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KS0gKFmiHAvq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(df[\"Usage\"].value_counts())  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zimrlUU8HEy3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(df.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4QYI2IoHWpL",
        "colab_type": "text"
      },
      "source": [
        "# 3- **Data Spliting**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9JGZRKCCAcq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train,train_y,X_test,test_y=[],[],[],[]  \n",
        "\n",
        "for index, row in df.iterrows():  \n",
        "    val=row['pixels'].split(\" \")  \n",
        "    try:  \n",
        "        if 'Training' in row['Usage']:  \n",
        "           X_train.append(np.array(val,'float32'))  \n",
        "           train_y.append(row['emotion'])  \n",
        "        elif 'PublicTest' in row['Usage']:  \n",
        "           X_test.append(np.array(val,'float32'))  \n",
        "           test_y.append(row['emotion'])  \n",
        "    except:  \n",
        "        print(f\"error occured at index :{index} and row:{row}\") "
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6333A5woSWX_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_features = 64  \n",
        "num_labels = 7  \n",
        "batch_size = 64  \n",
        "epochs = 30  \n",
        "width, height = 48, 48"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YsFBKUfkSYeB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = np.array(X_train,'float32')  \n",
        "train_y = np.array(train_y,'float32')  \n",
        "X_test = np.array(X_test,'float32')  \n",
        "test_y = np.array(test_y,'float32')  "
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcTmmZATSh5k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_y=np_utils.to_categorical(train_y, num_classes=num_labels)  \n",
        "test_y=np_utils.to_categorical(test_y, num_classes=num_labels) "
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehTTQE9YJF1Q",
        "colab_type": "text"
      },
      "source": [
        "#4- **Normalizing data between 0 and 1** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "TvqFi8-XCAcy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train -= np.mean(X_train, axis=0)  \n",
        "X_train /= np.std(X_train, axis=0)  \n",
        "X_test -= np.mean(X_test, axis=0)  \n",
        "X_test /= np.std(X_test, axis=0)  \n",
        "X_train = X_train.reshape(X_train.shape[0], 48, 48, 1)  \n",
        "X_test = X_test.reshape(X_test.shape[0], 48, 48, 1) "
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izbM6eEUKAV-",
        "colab_type": "text"
      },
      "source": [
        "# 5- **Designing the CNN**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQrgUo8fTbXz",
        "colab_type": "text"
      },
      "source": [
        "##5.1- **1st convolution layer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zt-vctO1Kb3o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()  \n",
        "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', input_shape=(X_train.shape[1:])))  \n",
        "model.add(Conv2D(64,kernel_size= (3, 3), activation='relu'))  \n",
        "# model.add(BatchNormalization())  \n",
        "model.add(MaxPooling2D(pool_size=(2,2), strides=(2, 2)))  \n",
        "model.add(Dropout(0.5))  "
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1x8QNZN3CAdF",
        "colab_type": "text"
      },
      "source": [
        "## 5.2 - **2nd Convolution Layer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNBdXxiHLfLW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.add(Conv2D(64, (3, 3), activation='relu'))  \n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))  \n",
        "# model.add(BatchNormalization())  \n",
        "model.add(MaxPooling2D(pool_size=(2,2), strides=(2, 2)))  \n",
        "model.add(Dropout(0.5)) "
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLfUeKr2UNe7",
        "colab_type": "text"
      },
      "source": [
        "## 5.3- **3rd Convolution Layer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUJ16arHaKMT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.add(Conv2D(128, (3, 3), activation='relu'))  \n",
        "model.add(Conv2D(128, (3, 3), activation='relu'))  \n",
        "# model.add(BatchNormalization())  \n",
        "model.add(MaxPooling2D(pool_size=(2,2), strides=(2, 2)))  \n",
        "model.add(Flatten())  "
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7EabEFjUl6t",
        "colab_type": "text"
      },
      "source": [
        "##5.4-  **Fully connected neural network**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_XHGn_oUvMG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.add(Dense(1024, activation='relu'))  \n",
        "model.add(Dropout(0.2))  \n",
        "model.add(Dense(1024, activation='relu'))  \n",
        "model.add(Dropout(0.2))  \n",
        "model.add(Dense(num_labels, activation='softmax'))  \n",
        "# model.summary()  "
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOCxPs8sU5aP",
        "colab_type": "text"
      },
      "source": [
        "#6-**Compliling the model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oi-DFb_OVBr4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss=categorical_crossentropy,  \n",
        "              optimizer=Adam(),  \n",
        "              metrics=['accuracy'])  "
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSM9ZdxfVIxE",
        "colab_type": "text"
      },
      "source": [
        "#7-**Training the model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02q29lEXVRjz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "outputId": "3eb8f3e8-016e-4668-a9b4-69b5d71f3a5c"
      },
      "source": [
        "model.fit(X_train, train_y,  \n",
        "          batch_size=batch_size,  \n",
        "          epochs=epochs,  \n",
        "          verbose=1,  \n",
        "          validation_data=(X_test, test_y),  \n",
        "          shuffle=True)  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 28709 samples, validate on 3589 samples\n",
            "Epoch 1/30\n",
            "28709/28709 [==============================] - 524s 18ms/step - loss: 1.7221 - accuracy: 0.2982 - val_loss: 1.4990 - val_accuracy: 0.4068\n",
            "Epoch 2/30\n",
            "28709/28709 [==============================] - 522s 18ms/step - loss: 1.4803 - accuracy: 0.4223 - val_loss: 1.3634 - val_accuracy: 0.4634\n",
            "Epoch 3/30\n",
            "28709/28709 [==============================] - 523s 18ms/step - loss: 1.3674 - accuracy: 0.4706 - val_loss: 1.2832 - val_accuracy: 0.5152\n",
            "Epoch 4/30\n",
            "28709/28709 [==============================] - 523s 18ms/step - loss: 1.3027 - accuracy: 0.4975 - val_loss: 1.2677 - val_accuracy: 0.5074\n",
            "Epoch 5/30\n",
            "28709/28709 [==============================] - 523s 18ms/step - loss: 1.2660 - accuracy: 0.5151 - val_loss: 1.2453 - val_accuracy: 0.5180\n",
            "Epoch 6/30\n",
            "28709/28709 [==============================] - 527s 18ms/step - loss: 1.2334 - accuracy: 0.5260 - val_loss: 1.2022 - val_accuracy: 0.5291\n",
            "Epoch 7/30\n",
            "28709/28709 [==============================] - 522s 18ms/step - loss: 1.2059 - accuracy: 0.5385 - val_loss: 1.1940 - val_accuracy: 0.5364\n",
            "Epoch 8/30\n",
            "28709/28709 [==============================] - 524s 18ms/step - loss: 1.1737 - accuracy: 0.5512 - val_loss: 1.1817 - val_accuracy: 0.5430\n",
            "Epoch 9/30\n",
            "28709/28709 [==============================] - 523s 18ms/step - loss: 1.1593 - accuracy: 0.5553 - val_loss: 1.1713 - val_accuracy: 0.5495\n",
            "Epoch 10/30\n",
            "28709/28709 [==============================] - 526s 18ms/step - loss: 1.1326 - accuracy: 0.5659 - val_loss: 1.1519 - val_accuracy: 0.5561\n",
            "Epoch 11/30\n",
            "28709/28709 [==============================] - 528s 18ms/step - loss: 1.1098 - accuracy: 0.5748 - val_loss: 1.1491 - val_accuracy: 0.5600\n",
            "Epoch 12/30\n",
            "28709/28709 [==============================] - 531s 18ms/step - loss: 1.0976 - accuracy: 0.5823 - val_loss: 1.1535 - val_accuracy: 0.5492\n",
            "Epoch 13/30\n",
            "27200/28709 [===========================>..] - ETA: 26s - loss: 1.0762 - accuracy: 0.5880"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKCwzVMnVaOm",
        "colab_type": "text"
      },
      "source": [
        "#8-**Saving the  model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-PUeUv-VglW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fer_json = model.to_json()  \n",
        "with open(\"fer.json\", \"w\") as json_file:  \n",
        "    json_file.write(fer_json)  \n",
        "model.save_weights(\"fer.h5\") "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNk8Y4m5b3tf",
        "colab_type": "text"
      },
      "source": [
        "#9-Detecting Real-Time Emotion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgAXg8iCb88r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os  \n",
        "import cv2  \n",
        "import numpy as np  \n",
        "from keras.models import model_from_json  \n",
        "from keras.preprocessing import image  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJbt4FopcEyh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#load model  \n",
        "model = model_from_json(open(\"fer.json\", \"r\").read())  \n",
        "#load weights  \n",
        "model.load_weights('fer.h5') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kb9gAA3fcMGx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "face_haar_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srFH4QN7cNW8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cap=cv2.VideoCapture(0)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxitWGrYcUIS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "while True:  \n",
        "    ret,test_img=cap.read()# captures frame and returns boolean value and captured image  \n",
        "    if not ret:  \n",
        "        continue  \n",
        "    gray_img= cv2.cvtColor(test_img, cv2.COLOR_BGR2GRAY)  \n",
        "\n",
        "    faces_detected = face_haar_cascade.detectMultiScale(gray_img, 1.32, 5)  \n",
        "\n",
        "\n",
        "    for (x,y,w,h) in faces_detected:  \n",
        "        cv2.rectangle(test_img,(x,y),(x+w,y+h),(255,0,0),thickness=7)  \n",
        "        roi_gray=gray_img[y:y+w,x:x+h]#cropping region of interest i.e. face area from  image  \n",
        "        roi_gray=cv2.resize(roi_gray,(48,48))  \n",
        "        img_pixels = image.img_to_array(roi_gray)  \n",
        "        img_pixels = np.expand_dims(img_pixels, axis = 0)  \n",
        "        img_pixels /= 255  \n",
        "\n",
        "        predictions = model.predict(img_pixels)  \n",
        "\n",
        "        #find max indexed array  \n",
        "        max_index = np.argmax(predictions[0])  \n",
        "\n",
        "        emotions = ('angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral')  \n",
        "        predicted_emotion = emotions[max_index]  \n",
        "\n",
        "        cv2.putText(test_img, predicted_emotion, (int(x), int(y)), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)  \n",
        "\n",
        "    resized_img = cv2.resize(test_img, (1000, 700))  \n",
        "    cv2.imshow('Facial emotion analysis ',resized_img)  \n",
        "\n",
        "\n",
        "\n",
        "    if cv2.waitKey(10) == ord('q'):#wait until 'q' key is pressed  \n",
        "        break  \n",
        "\n",
        "cap.release()  \n",
        "cv2.destroyAllWindows  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vm7uTbEWCAei",
        "colab_type": "text"
      },
      "source": [
        "References:\n",
        "\n",
        "[1] Building a Real Time Emotion Detection with Python\n",
        "\n",
        "https://morioh.com/p/801c509dda99?f=5c21f93bc16e2556b555ab2f\n",
        "\n"
      ]
    }
  ]
}